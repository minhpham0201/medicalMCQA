{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1rgHc7f5yBb"
      },
      "outputs": [],
      "source": [
        "# to be deleted\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "def translate_text(text):\n",
        "    translator = GoogleTranslator(source='vi', target='en')\n",
        "    try:\n",
        "        trans_text = translator.translate(text)\n",
        "        return trans_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"Error: \" + str(e)\n",
        "\n",
        "# Use this to map translating only for test hf dataset\n",
        "def translate_text_google(example):\n",
        "    question = example['question']\n",
        "    op1 = example['option_1']\n",
        "    op2 = example['option_2']\n",
        "    op3 = example['option_3']\n",
        "    op4 = example['option_4']\n",
        "\n",
        "    return {\n",
        "        'trans_question': translate_text(question),\n",
        "        'trans_option_1': translate_text(op1),\n",
        "        'trans_option_2': translate_text(op2),\n",
        "        'trans_option_3': translate_text(op3),\n",
        "        'trans_option_4': translate_text(op4)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to be deleted\n",
        "from transformers import pipeline\n",
        "\n",
        "def summarize_text(text,min_length=220, max_length=400):\n",
        "    summarizer = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
        "    summary = summarizer(text, max_length=max_length, min_length=min_length)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "G9EU55Ft5481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to be deleted\n",
        "\n",
        "def get_context_from_question_faiss(question,k,faiss_index_path,trans_corpus_path):\n",
        "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    index = faiss.read_index(faiss_index_path)\n",
        "    trans_corpus = pd.read_csv(trans_corpus_path)\n",
        "\n",
        "    xq = model.encode(question)\n",
        "    xq = np.expand_dims(xq, axis=0)\n",
        "    D, I = index.search(xq, k)\n",
        "\n",
        "    return [{trans_corpus['Url'][i]: trans_corpus['Content'][i]} for i in I[0]]"
      ],
      "metadata": {
        "id": "6mCS0FB76D9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demo, to be deleted\n",
        "question = 'What high technology is used to treat hormonal acne?'\n",
        "get_context_from_question_faiss(question, k=2,faiss_index_path = '/content/drive/MyDrive/faiss_768.index',\n",
        "                                                trans_corpus_path = '/content/drive/MyDrive/df_768.csv')"
      ],
      "metadata": {
        "id": "kQdvblUB6FwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to be deleted\n",
        "def dataset_get_context(example):\n",
        "    id = example['id']\n",
        "    level = id.split('_')[0]\n",
        "    question = example['trans_ner_question']\n",
        "\n",
        "    if level == 'level1':\n",
        "        k=1\n",
        "    else:\n",
        "        k=2\n",
        "\n",
        "    context = get_context_from_question_faiss(question=question, k=k,\n",
        "                                                    faiss_index_path = '/content/drive/MyDrive/faiss_768.index',\n",
        "                                                    trans_corpus_path = '/content/drive/MyDrive/df_768.csv')\n",
        "    concat_context = \"\\n\".join(context) # Concat all text in to long\n",
        "    example['context'] = concat_context\n",
        "    return example"
      ],
      "metadata": {
        "id": "l6WR9lMb6HVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use this for batch faiss encode\n",
        "\n",
        "faiss_index_path = '/content/drive/MyDrive/faiss_768.index'\n",
        "trans_corpus_path = '/content/drive/MyDrive/df_768.csv'\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "trans_corpus = pd.read_csv(trans_corpus_path)\n",
        "\n",
        "def dataset_get_context_batch(examples):\n",
        "    id_batch = examples['id']\n",
        "    level_batch = [id.split('_')[0] for id in id_batch]\n",
        "    question_batch = examples['trans_ner_question']\n",
        "\n",
        "    ## batch sentenceTransformers\n",
        "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    xq_batch = model.encode(question_batch)\n",
        "    context_batch = []\n",
        "    for xq, level in zip(xq_batch, level_batch):\n",
        "        if level == 'level1':\n",
        "            k=1\n",
        "        else:\n",
        "            k=2\n",
        "        xq = np.expand_dims(xq, axis=0)  # query vector\n",
        "        D, I = index.search(xq, k)   # D: distance, I: index\n",
        "        context = [trans_corpus['Content'][i] for i in I[0]]\n",
        "        concat_context = \"\\n\".join(context) # Concat all text in to long text\n",
        "\n",
        "        context_batch.append(concat_context)\n",
        "\n",
        "    examples['context'] = context_batch\n",
        "    return examples# MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7  --- 270M params\n",
        "# MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli   --- 435M params\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\", device=0, use_fast=True)\n",
        "\n",
        "def get_binary_answer(choices: list, answers: list):\n",
        "    length = len(choices)\n",
        "    result = ['0']*length\n",
        "    for answer in answers:\n",
        "        for i in range(len(choices)):\n",
        "            if answer == choices[i]:\n",
        "                result[i] = '1'\n",
        "    return ''.join(result)\n",
        "\n",
        "\n",
        "def get_prediction(example):\n",
        "    prompt = example['prompt']\n",
        "\n",
        "    id = example['id']\n",
        "    choices = [example['trans_option_1'], example['trans_option_2'],\n",
        "               example['trans_option_3'], example['trans_option_4'], example['trans_option_5']]\n",
        "\n",
        "    choices = [item for item in choices if item != 'Cloudy sky']\n",
        "\n",
        "    predicted_labels = []\n",
        "    threshold = 0.85\n",
        "\n",
        "    results = classifier(prompt, choices, multi_label=True)\n",
        "    scores = results['scores']\n",
        "    labels = results['labels']\n",
        "\n",
        "    highest_score_index = np.argmax(scores)\n",
        "    my_answers = [labels[highest_score_index]]\n",
        "\n",
        "    # Include labels with scores above the threshold\n",
        "    for i in range(len(labels)):\n",
        "        if (scores[i] >= threshold) and (labels[i] not in my_answers):\n",
        "            my_answers.append(labels[i])\n",
        "\n",
        "    binary_answer = get_binary_answer(choices, my_answers)\n",
        "    binary_answer = f'{id},{binary_answer}'\n",
        "    example['binary_answer'] = binary_answer\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "dxGWvub56KnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get prediction using multiple context\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\", device=0, use_fast=True)\n",
        "\n",
        "def get_binary_answer(choices: list, answers: list):\n",
        "    length = len(choices)\n",
        "    result = ['0']*length\n",
        "    for answer in answers:\n",
        "        for i in range(len(choices)):\n",
        "            if answer == choices[i]:\n",
        "                result[i] = '1'\n",
        "    return ''.join(result)\n",
        "\n",
        "\n",
        "def get_multi_context_prediction(example):\n",
        "    prompt_1 = example['prompt_1']\n",
        "    prompt_2 = example['prompt_2']\n",
        "    id = example['id']\n",
        "    choices = [example['trans_option_1'], example['trans_option_2'],\n",
        "               example['trans_option_3'], example['trans_option_4'], example['trans_option_5']]\n",
        "\n",
        "    choices = [item for item in choices if item != 'Cloudy sky']\n",
        "\n",
        "    threshold = 0.85\n",
        "\n",
        "    results = classifier([prompt_1, prompt_2], choices, multi_label=True)\n",
        "    scores_1, labels_1 = results[0]['scores'], results[0]['labels']\n",
        "    scores_2, labels_2 = results[1]['scores'], results[1]['labels']\n",
        "\n",
        "    max_score_1_index = np.argmax(scores_1)\n",
        "    my_answers_1 = [labels_1[max_score_1_index]]\n",
        "\n",
        "    max_score_2_index = np.argmax(scores_2)\n",
        "    my_answers_2 = [labels_2[max_score_2_index]]\n",
        "\n",
        "    # Include labels with scores above the threshold\n",
        "    for i in range(len(choices)):\n",
        "        if (scores_1[i] >= threshold) and (labels_1[i] not in my_answers_1):\n",
        "            my_answers_1.append(labels_1[i])\n",
        "\n",
        "        if (scores_2[i] >= threshold) and (labels_2[i] not in my_answers_2):\n",
        "            my_answers_2.append(labels_2[i])\n",
        "\n",
        "    binary_answer_1 = get_binary_answer(choices, my_answers_1)\n",
        "    binary_answer_2 = get_binary_answer(choices, my_answers_2)\n",
        "\n",
        "    binary_answer_1 = f'{id},{binary_answer_1}'\n",
        "    binary_answer_2 = f'{id},{binary_answer_2}'\n",
        "    example['binary_answer_1'] = binary_answer_1\n",
        "    example['binary_answer_2'] = binary_answer_2\n",
        "    return example"
      ],
      "metadata": {
        "id": "pgsuEHvq6KhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to be deleted when done\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def translate_term_for_mapping_batch(examples):\n",
        "    ########### model AI translate\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model_name = '/content/drive/MyDrive/Model_Translation_envit5'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    term_batch = examples['Name']\n",
        "    term_batch_prepend = [f'vi : {term}' for term in term_batch]\n",
        "\n",
        "    inputs = tokenizer(term_batch_prepend, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=56)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(**inputs, min_length=0,max_length=56,no_repeat_ngram_size=2)\n",
        "\n",
        "    translated_terms = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    trans_terms = [term.replace('en:','').replace('vi:','').strip() for term in translated_terms]\n",
        "\n",
        "    examples['model_trans_term']= trans_terms\n",
        "\n",
        "    ########### Google Translate\n",
        "    ggtrans_terms = []\n",
        "    for term in term_batch:\n",
        "        ggtrans_term = translate_text(term)\n",
        "        ggtrans_terms.append(ggtrans_term)\n",
        "\n",
        "    examples['gg_trans_term']= ggtrans_terms\n",
        "\n",
        "    return examples"
      ],
      "metadata": {
        "id": "HsqZGx3NZ5Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_example(example):\n",
        "    question = example['question']\n",
        "    context = example['context']\n",
        "    opa = example['trans_option_1']\n",
        "    opb = example['trans_option_2']\n",
        "    opc = example['trans_option_3']\n",
        "    opd = example['trans_option_4']\n",
        "\n",
        "    if opd != '':\n",
        "        n_choice = 4\n",
        "    elif opc != '':\n",
        "        n_choice = 3\n",
        "    elif opb != '':\n",
        "        n_choice = 2\n",
        "\n",
        "    options = [opa, opb, opc, opd][:n_choice]\n",
        "    input = [f'question: {question}, answer: {option}, context: {context}' for option in [opa, opb, opc, opd][:n_choice]]\n",
        "\n",
        "    tokenized = tokenizer(input, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    input_ids = tokenized['input_ids'].unsqueeze(dim=0)\n",
        "    attention_mask = tokenized['attention_mask'].unsqueeze(dim=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "    example['predicted_labels'] = predicted_labels.tolist()\n",
        "\n",
        "    return example\n",
        "\n",
        "prediction = test_ggtrans_context.map(predict_example)"
      ],
      "metadata": {
        "id": "1ea5ghDidsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hard vote\n",
        "def get_binary_answer(choices: list, answers: list):\n",
        "    length = len(choices)\n",
        "    result = ['0']*length\n",
        "    for answer in answers:\n",
        "        for i in range(len(choices)):\n",
        "            if answer == choices[i]:\n",
        "                result[i] = '1'\n",
        "    return ''.join(result)\n",
        "\n",
        "def ensemble_binary(scores: list, vote=2):\n",
        "    length = len(scores[0])\n",
        "    answer = ['0']*length\n",
        "    for i in range(len(scores[0])):\n",
        "        count = 0\n",
        "        for j in range(len(scores)):\n",
        "          if scores[j][i] == '1':\n",
        "            count += 1\n",
        "          if count == vote:\n",
        "            answer[i] = '1'\n",
        "    return ''.join(answer)\n",
        "\n",
        "def final_ensemble(scores):\n",
        "    length = len(scores[0])\n",
        "    answer = ['0']*length\n",
        "    for i in range(len(scores[0])):\n",
        "        for j in range(len(scores)):\n",
        "          if scores[j][i] == '1':\n",
        "            answer[i] = '1'\n",
        "            break\n",
        "    return ''.join(answer)\n",
        "\n",
        "def get_multi_context_prediction_ensemble_hard_vote(example):\n",
        "    prompt_1 = example['prompt_1']\n",
        "    prompt_2 = example['prompt_2']\n",
        "    id = example['id']\n",
        "    choices = [example['trans_option_1'], example['trans_option_2'],\n",
        "               example['trans_option_3'], example['trans_option_4'], example['trans_option_5']]\n",
        "\n",
        "    choices = [item for item in choices if item != 'cloudy sky']\n",
        "\n",
        "    threshold = 0.87\n",
        "    argmax_threshold = 0.67\n",
        "    context1_for_ensemble = [] # result will be as binary : ['1001', '1001','1000',.....,'0001'] then used to merge into final answer\n",
        "    context2_for_ensemble = []\n",
        "\n",
        "    for classifier in classifiers:\n",
        "        results = classifier([prompt_1, prompt_2], choices, multi_label=True)\n",
        "        context1_scores, context1_labels = results[0]['scores'], results[0]['labels']\n",
        "        context2_scores, context2_labels = results[1]['scores'], results[1]['labels']\n",
        "\n",
        "        candidate1_answer = []\n",
        "        candidate2_answer = []\n",
        "        if np.max(context1_scores) >= argmax_threshold: # make sure max score > threshold\n",
        "            max_score_1_index = np.argmax(context1_scores)\n",
        "            candidate1_answer.append(context1_labels[max_score_1_index])\n",
        "\n",
        "        if np.max(context2_scores) >= argmax_threshold: # make sure max score > threshold\n",
        "            max_score_2_index = np.argmax(context2_scores)\n",
        "            candidate2_answer.append(context2_labels[max_score_2_index])\n",
        "\n",
        "        # Include labels with scores above the threshold\n",
        "        for i in range(len(choices)):\n",
        "            if (context1_scores[i] >= threshold) and (context1_labels[i] not in candidate1_answer):\n",
        "                candidate1_answer.append(context1_labels[i])\n",
        "\n",
        "            if (context2_scores[i] >= threshold) and (context2_labels[i] not in candidate2_answer):\n",
        "                candidate2_answer.append(context2_labels[i])\n",
        "\n",
        "        bin_candidate1_answer = get_binary_answer(choices, candidate1_answer)\n",
        "        bin_candidate2_answer = get_binary_answer(choices, candidate2_answer)\n",
        "\n",
        "        context1_for_ensemble.append(bin_candidate1_answer)\n",
        "        context2_for_ensemble.append(bin_candidate2_answer)\n",
        "\n",
        "    # Begin to ensemble answer\n",
        "    vote = 3\n",
        "    context1_final_answer = ensemble_binary(context1_for_ensemble, vote=vote)\n",
        "    context2_final_answer = ensemble_binary(context2_for_ensemble, vote=vote)\n",
        "\n",
        "    # Final ensemble\n",
        "    final_answer = final_ensemble([context1_final_answer, context2_final_answer])\n",
        "    binary_final_answer = f'{id},{final_answer}'\n",
        "\n",
        "    example['context1_for_ensemble'] = context1_for_ensemble\n",
        "    example['context2_for_ensemble'] = context2_for_ensemble\n",
        "    example['context1_answer'] = context1_final_answer\n",
        "    example['context2_answer'] = context2_final_answer\n",
        "    example['binary_final_answer'] = binary_final_answer\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "QNF4nWAnbCKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxOvVZZhbCBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}